SIMPLE CONTRASTIVE LOSS

mask = 

1,0,0,0
0,1,0,0
0,0,1,0
0,0,0,1

labels = 

1,0,0,0,0,0,0,0
0,1,0,0,0,0,0,0
0,0,1,0,0,0,0,0
0,0,0,1,0,0,0,0



logits_aa = 

S11-LN, S12   , S13   , S14
S21   , S22-LN, S23   , S24
S31   , S32   , S33-LN, S34
S41   , S42   , S43   , S44-LN



logits_bb (1 <=> 5) (2 <=> 6)

S55-LN, S56   , S57   , S58
S65   , S66-LN, S67   , S68
S75   , S76   , S77-LN, S78
S85   , S86   , S87   , S88-LN


logits_ab = 

S15, S16, S17, S18
S25, S26, S27, S28
S35, S36, S37, S38
S45, S46, S47, S48

concat_ab_aa = 

S15, S16, S17, S18, S11-LN, S12   , S13   , S14
S25, S26, S27, S28, S21   , S22-LN, S23   , S24
S35, S36, S37, S38, S31   , S32   , S33-LN, S34
S45, S46, S47, S48, S41   , S42   , S43   , S44-LN


loss_a = tf.nn.softmax_cross_entropy_with_logits(
        labels, concat_ab_aa)

labels with concat(ogits_ab, logits_aa) in softmax_X_ent:

-log(exp(S15)/Sexp(S1i)),0,0,0,0,0,0,0
0,-log(exp(S26)/Sexp(S2i),0,0,0,0,0,0
0,0,-log(exp(S37)/Sexp(S3i),0,0,0,0,0
0,0,0,-log(exp(S48)/Sexp(S4i),0,0,0,0



loss_b = tf.nn.softmax_cross_entropy_with_logits(
        labels, tf.concat([logits_ba, logits_bb], 1))

loss = tf.reduce_mean(loss_a + loss_b)



CONTRASTIVE LOSS WITH OVERSAMPLING

